name: Weekly Job Scraper

# IMPORTANT: This workflow only runs on schedule (Monday 8am) or manual trigger
# For code changes that need report updates, use update_report.yml instead (no scraping)
on:
  schedule:
    - cron: '0 8 * * 1' # Runs at 08:00 UTC every Monday
  workflow_dispatch:      # Allows you to click "Run Now" button manually
  # DO NOT add push triggers here - use update_report.yml for code changes

permissions:
  contents: write
  pages: write
  id-token: write

jobs:
  scrape_and_update:
    runs-on: ubuntu-latest
    timeout-minutes: 240  # 4 hours max (conservative scraping with 5-min cooldowns)

    steps:
      - name: Checkout Code with LFS
        uses: actions/checkout@v4
        with:
          lfs: true  # Enable Git LFS for database file

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip' # caching pip dependencies

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          python -m pip install -r requirements.txt
          playwright install chromium
          python -m pip list

      - name: Debug Environment
        run: |
          echo "Checking scraper.py content (first 15 lines):"
          head -n 15 scraper.py
          echo "Checking tqdm installation:"
          python -c "import tqdm; print(tqdm.__version__)" || echo "tqdm import failed"

      - name: Run Scraper
        timeout-minutes: 210  # 3.5 hours for scraping (leaves buffer for report generation)
        run: |
          python scraper.py

      - name: Discover and Update Tech Whitelist
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        run: |
          python -m tools.whitelist_discovery --min-count 5 --live

      - name: Generate Static Report
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        run: |
          python generate_report.py

      - name: Generate Visual Trends
        run: |
          python visualizer.py

      - name: Update README Stats
        run: |
          python update_readme.py

      - name: Commit and Push Data
        run: |
          git config --global user.name 'Scraper Bot'
          git config --global user.email 'bot@noreply.github.com'

          # Add database (tracked via Git LFS), CSVs, report, and README
          git add data/intelligence.db classifiers.py report/whitelist_candidates.md data/*.csv public/ README.md || echo "No files to add"

          # Only commit if there are changes
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            git commit -m "Auto-update: Job market data and report [$(date +'%Y-%m-%d')] [skip ci]"
            # Pull before pushing to handle cases where the repo changed during the scrape
            git pull --rebase origin main
            git push origin main
          fi

      - name: Deploy to GitHub Pages
        uses: actions/upload-pages-artifact@v3
        with:
          path: './public'

  deploy:
    needs: scrape_and_update
    permissions:
      pages: write
      id-token: write
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    runs-on: ubuntu-latest
    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
